from langgraph.graph import StateGraph, END
from typing import Optional, TypedDict, Literal, Union
from langchain_ollama import ChatOllama
import json, uuid
from langgraph.types import interrupt, Command
#input trace
trace={
    "input_request": {
        "args": [
            {
            "input": "I want a high performance compute with H1000 GPU and lots of memory."
            }
        ]
    },
    "output_response": {
        "use_case": "High-performance computing",
        "performance_level": "high",
        "budget_hints": "{'anyOf': [{'type': 'string'}, {'type': 'null'}]}",
        "specific_requirements": [
            "H1000 GPU",
            "lots of memory"
        ],
        "gpu_preference": "H1000",
        "memory_needs": "lots of memory",
        "storage_needs": "{'anyOf': [{'type': 'string'}, {'type': 'null'}]}",
        "features_needed": [
            "{\"anyOf\": [{\"type\": \"string\"}",
            "{\"type\": \"null\"}]}"
        ],
        "urgency": "{'anyOf': [{'type': 'string'}, {'type': 'null'}]}"
    }

}


# Output format
class Intent(TypedDict):
    augmented: bool
    input_text: str 
    reasoning: str
    use_case: str
    performance_level: str
    budget_hints: Optional[str]
    specific_requirements: str
    gpu_preference: Optional[str]
    memory_needs: Optional[str]
    storage_needs: Optional[str]
    features_needed: str
    urgency: Optional[str]

#Agent Inbox - Interrupt related classes
class HumanInterruptConfig(TypedDict):
    allow_ignore: bool
    allow_respond: bool
    allow_edit: bool
    allow_accept: bool


class ActionRequest(TypedDict):
    action: str
    args: dict

class HumanInterrupt(TypedDict):
    action_request: ActionRequest
    config: HumanInterruptConfig
    description: Optional[str]


class HumanResponse(TypedDict):
    type: Literal['accept', 'ignore', 'response', 'edit']
    args: Union[None, str, ActionRequest]
    

def generate_reflection(state:Intent) -> Intent:

    with open('intent.json','r') as f:
        object=json.load(f)
    signature=object['predict']['signature']
    print("\nGENERATING INITIAL REFLECTION\n")
    prompt=f'''You are an expert on IT systems and are tasked with generating a detailed reflection based on the previous configuration generated.
    You are given a user query and the product designed using a configuration tool. Think and generate a detailed reflection on 
    reasoning behind the request, the primary use case (e.g., AI training, inference, data processing), required performance level (high, medium, low),
    any budget hints, specific technical requirements mentioned, GPU preferences, memory needs, storage needs, desired features, and urgency of deployment.
    Provide a clear and detailed response that would be better than the trace provided below, and that captures all necessary elements for effective GPU configuration.

    trace: {trace}

    generate reflection in the format which has the signature as shown below:
    {signature}
    '''

    response=llm.invoke(prompt)
    return response

# todo : fix edit-response node
def generate_edited_reflection(state:Intent) -> Intent:

    print("\nGENERATING EDITED REFLECTION\n")
    prompt=f'''You are an expert on IT systems and are tasked with generating a detailed reflection based on the previous configuration generated.
    You are given a user query and the product designed using a configuration tool, along with previous reflection generated by you. The user has
    decided that there needs changes to your reflection, hence think and generate a detailed reflection on 
    reasoning behind the request, the primary use case (e.g., AI training, inference, data processing), required performance level (high, medium, low),
    any budget hints, specific technical requirements mentioned, GPU preferences, memory needs, storage needs, desired features, and urgency of deployment.
    Provide a clear and detailed response that would be better than the trace and reflection provided below, and that captures all necessary elements for effective GPU configuration.

    trace: {trace}
    reflection generated previously: {state}
    '''

    response=llm.invoke(prompt)   
    return response


def human_node(state: Intent):
    print("\nHUMAN INTERRUPT\n")
    #Create an interrupt
    request: HumanInterrupt = {
        "action_request": {
            "action": 'Accept/Edit response',
            "args": state
        },
        "config": {
            "allow_ignore": True,
            "allow_respond": False,
            "allow_edit": True,
            "allow_accept": True
        },
        "description": None # Generate a detailed markdown description.
    }

    # Send the interrupt request, and extract the first response.
    response = interrupt(request)
    if response[0]['type'] == "accept":
        return Command(goto='finish')
    elif response[0]['type'] == "edit":
        return Command(goto='finish')
    elif response[0]['type']=="ignore":
        return Command(goto=END)

#Final node after accept
def writeback(state: Intent):
    print("\nWRITING BACK\n")
    state_dict=state
    with open('intent.json', 'r') as file:
        intent_output=json.load(file)

    intent_output['predict']['demos'].append(state_dict)
    intent_output['predict']['traces'].append(trace)

    with open('intent.json', 'w') as file:
        json.dump(intent_output,file,indent=4)
    return state

        
llm=ChatOllama(model="llama3.2:latest").with_structured_output(Intent)    

graph_builder=StateGraph(Intent)

graph_builder.add_node('reflect',generate_reflection)
graph_builder.add_node('human',human_node)
graph_builder.add_node('edit_reflection',generate_edited_reflection)
graph_builder.add_node('finish',writeback)

graph_builder.set_entry_point('reflect')
graph_builder.add_edge('reflect','human')
graph_builder.add_edge('edit_reflection','human')
graph_builder.add_edge('finish',END)
graph=graph_builder.compile()

state = Intent()
config={"configurable":{"thread_id":uuid.uuid4()}}
result=graph.invoke(state)





