from langgraph.graph import StateGraph, END, START
from typing import Optional, TypedDict, Literal, Union
from langchain_ollama import ChatOllama
import json, uuid
from langgraph.types import interrupt, Command
#input trace
trace={
    "input_request": {
        "args": [
            {
            "input": "I want a high performance compute with H1000 GPU and lots of memory."
            }
        ]
    },
    "output_response": {
        "use_case": "High-performance computing",
        "performance_level": "high",
        "budget_hints": "{'anyOf': [{'type': 'string'}, {'type': 'null'}]}",
        "specific_requirements": [
            "H1000 GPU",
            "lots of memory"
        ],
        "gpu_preference": "H1000",
        "memory_needs": "lots of memory",
        "storage_needs": "{'anyOf': [{'type': 'string'}, {'type': 'null'}]}",
        "features_needed": [
            "{\"anyOf\": [{\"type\": \"string\"}",
            "{\"type\": \"null\"}]}"
        ],
        "urgency": "{'anyOf': [{'type': 'string'}, {'type': 'null'}]}"
    }
}
human_response=""
input_text=trace["input_request"]['args'][0]['input']
refelctions=[]
previous_state={}

print(input_text)
# Output format
class Intent(TypedDict):
    augmented: bool
    input_text: str
    reasoning: str
    use_case: str
    performance_level: str
    budget_hints: Optional[str]
    specific_requirements: str
    gpu_preference: Optional[str]
    memory_needs: Optional[str]
    storage_needs: Optional[str]
    features_needed: str
    urgency: Optional[str]

class Reflection(TypedDict):
    reflection: str

#Agent Inbox - Interrupt related classes
class HumanInterruptConfig(TypedDict):
    allow_ignore: bool
    allow_respond: bool
    allow_edit: bool
    allow_accept: bool


class ActionRequest(TypedDict):
    action: str
    args: dict

class HumanInterrupt(TypedDict):
    action_request: ActionRequest
    config: HumanInterruptConfig
    description: Optional[str]


class HumanResponse(TypedDict):
    type: Literal['accept', 'ignore', 'response', 'edit']
    args: Union[None, str, ActionRequest]
    
# first node: generate iitial reflection based on original trace
def generate_intitial_reflection(state: Intent) -> Intent:

    with open('intent.json','r') as f:
        object=json.load(f)
    signature=object['predict']['signature']
    print("\nGENERATING INITIAL REFLECTION\n")
    prompt=f'''You are an expert on IT systems and are tasked with generating a detailed reflection on why the configuration generated previously 
    did not match well with the user's request.
    You are given the user query and the product designed using a configuration tool. Think and generate a detailed reflection on 
    reasoning behind the request, the primary use case (e.g., AI training, inference, data processing), required performance level (high, medium, low),
    any budget hints, specific technical requirements mentioned, GPU preferences, memory needs, storage needs, desired features, and urgency of deployment.
    Provide a clear and detailed response that would be better than the trace provided below, and that captures all necessary elements for effective GPU configuration.

    trace: {trace}

    Keep the input text as it is, and generate new output in the format which has the signature as shown below, and your reflection would be written in the "reasoning" attribute:
    {signature}
    '''

    response=llm_trace_generator.invoke(prompt)
    print("\n\nINITIAL REFLECTION DONE")

    return response

# generate reflection based on user's edited trace
def generate_edited_reflection(state:Intent) -> Intent:

    print("\nGENERATING EDITED REFLECTION\n")
    prompt=f'''You are an expert on IT systems and are tasked with generating a detailed reflection on why the user has chosen a better configuration than the one
    you had generated previously.
    You are given a user query and the product designed by you, along with previous reflection generated by you. The user has
    decided that there needs changes to your configuration, hence think and generate a detailed reflection on 
    reasoning behind the change in configuration, reasoning behind the request, the primary use case (e.g., AI training, inference, data processing), required performance level (high, medium, low),
    any budget hints, specific technical requirements mentioned, GPU preferences, memory needs, storage needs, desired features, and urgency of deployment.
    Provide a clear and detailed response that would help in understanding the user's choice.

    Reflection generated previously: {previous_state}
    
    The user's edited and better configuration: {state}
    '''

    response=llm_reflector.invoke(prompt)  
    print("\n",response,"\n") 
    
    print("\n\nGENERATING EDITED REFLECTION DONE")
    return {**state, "reasoning":response['reflection']}


# generate new reflection based on human response during interrupt
def generate_new_reflection(state:Intent) -> Intent:

    print("\nGENERATING NEW REFLECTION\n")
    with open('intent.json','r') as f:
        object=json.load(f)
    signature=object['predict']['signature']
    prompt=f'''You are an expert on IT systems and are tasked with generating a detailed reflection and output based on the user's response to your generated configuration.
    You are given a user query and the product you designed as a configuration tool, along with previous reflection generated by you. The user has
    decided that there needs changes to your configuration, hence think and generate a detailed reflection on 
    reasoning behind the request, the primary use case (e.g., AI training, inference, data processing), required performance level (high, medium, low),
    any budget hints, specific technical requirements mentioned, GPU preferences, memory needs, storage needs, desired features, and urgency of deployment.
    Provide a clear and detailed response that would be better than the configuration and reflection provided below, taking in the user's suggestion and expectation on making 
    a better configuration, one that captures all necessary elements for effective GPU configuration.
    The user has suggested and requested with: {human_response}

    Configuration and reasoning generated previously by you: {state}

    Keep the input text as it is, and generate new output in the format which has the signature as shown below, and your reflection would be written in the "reasoning" attribute:
    {signature}
    '''

    response=llm_trace_generator.invoke(prompt)   
    print("\n\nGENERATING NEW REFLECTION DONE")
    return response

# interrupt node
def human_node(state: Intent)->Command[Literal["finish","edit_reflection","new_reflection"]]:
    global human_response
    global previous_state
    print("\nHUMAN INTERRUPT\n")
    #Create an interrupt
    request: HumanInterrupt = {
        "action_request": {
            "action": 'Accept/Edit response',
            "args": state
        },
        "config": {
            "allow_ignore": True,
            "allow_respond": True,
            "allow_edit": True,
            "allow_accept": True
        },
        "description": None #Generate a detailed markdown description.
    }

    #Send the interrupt request, and extract the first response.
    while True:
        response = interrupt(request)
        if(response!=""):
            print("\n\nresponse: ",response)
            if response[0]['type'] == "accept":
                # writes back the output into 'traces' of intent.json
                print("\nACCEPTED\n")
                return Command(goto='finish')
            elif response[0]['type']=="response":
                # goes to 'generate_new_reflection'
                print("\nRESPONDED\n")
                human_response=response[0]['args']
                return Command(goto='new_reflection')
            elif response[0]['type'] == "edit":
                # goes to 'generate_edited_reflection
                print("\nEDITED\n")
                previous_state=state
                state=Intent(response[0]['args']['args'])
                return Command(goto='edit_reflection',update=state)
            elif response[0]['type']=="ignore":
                # ignores reflection and trace
                print("\nIGNORED\n")
                return Command(goto=END)
        else:
            print("\nNO RESPONSE\nRESUMING INTERRUPT\n")

#Final node after accept
def writeback(state: Intent):
    print("\nWRITING BACK\n")
    # state_dict=state
    with open('intent.json', 'r') as file:
        intent_output=json.load(file)

    intent_output['predict']['traces'].append(state)

    with open('intent.json', 'w') as file:
        json.dump(intent_output,file,indent=4)
    print("\nDONE WRITING BACK\n")
    return state
    


llm=ChatOllama(model="llama3.2:latest")
llm_reflector=llm.with_structured_output(Reflection)
llm_trace_generator=llm.with_structured_output(Intent)    

graph_builder=StateGraph(Intent)

graph_builder.add_node('initial_reflection',generate_intitial_reflection)
graph_builder.add_node('human',human_node)
graph_builder.add_node('edit_reflection',generate_edited_reflection)
graph_builder.add_node('new_reflection',generate_new_reflection)
graph_builder.add_node('finish',writeback)


graph_builder.add_edge(START,'initial_reflection')
graph_builder.add_edge('initial_reflection','human')
graph_builder.add_edge('new_reflection','human')
graph_builder.add_edge('edit_reflection','finish')
graph_builder.add_edge('finish',END)
graph=graph_builder.compile()

state = Intent()
config={"configurable":{"thread_id":uuid.uuid4()}}
print("\nINVOKING GRAPH\n")
result=graph.invoke(state)
print("\nDONE W GRAPH\n")






